% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/misc_functions.R
\name{write_parquet_dt}
\alias{write_parquet_dt}
\title{Write data.table to Parquet with key metadata preservation}
\usage{
write_parquet_dt(
  x,
  path,
  keys = NULL,
  partitioning = NULL,
  compression = "snappy"
)
}
\arguments{
\item{x}{Either a data.table object OR a character path to an fst file.
If a path is provided, the fst file is read as a data.table.}

\item{path}{Character scalar. The output path for the parquet file or
directory (if partitioning is used).}

\item{keys}{Specifies which columns to use as data.table keys. Can be:
\itemize{
  \item `NULL` (default): Uses existing keys from the data.table
  \item A character vector of column names: Uses these as keys
  \item `"impactncd"`: Special mode for IMPACTncd project - automatically
    determines keys by excluding distribution parameters (mu, sigma, nu,
    tau, maxq, minq) and columns ending with digits, then sorts
    alphabetically with "year" first if present
}}

\item{partitioning}{Optional character vector of column names to use for
Hive-style partitioning. If `NULL` (default), writes a single parquet file.
If specified, creates a directory structure with partition folders.
If `TRUE`, automatically partitions by "year" if that column exists.}

\item{compression}{Character scalar. Compression codec to use.
Default is "snappy". Other options include "gzip", "zstd", "lz4", "uncompressed".}
}
\value{
Invisibly returns the path to the written parquet file/directory.
}
\description{
Writes a data.table (or reads from an fst file) to Parquet format while
preserving data.table key information in the parquet metadata. The keys
are stored as JSON in the `r.data.table.keys` metadata field and can be
restored when reading with [read_parquet_dt()].
}
\details{
The function stores data.table keys in the parquet schema metadata under
the field `r.data.table.keys` as a JSON-encoded character vector. This
metadata is read by [read_parquet_dt()] to automatically restore keys.

When `partitioning` is specified, the data is written using
`arrow::write_dataset()` which creates a Hive-style partitioned directory.
Otherwise, `arrow::write_parquet()` is used for a single file.

The `"impactncd"` key mode is designed for the IMPACTncd simulation project
where data tables typically have distribution parameters (mu, sigma, nu, tau)
and quantile columns that should not be used as keys.
}
\examples{
\dontrun{
library(data.table)

# Create a keyed data.table
dt <- data.table(id = 1:100, year = rep(2020:2024, 20), value = rnorm(100))
setkey(dt, id, year)

# Write to parquet (keys preserved automatically)
write_parquet_dt(dt, "output.parquet")

# Read back with keys restored
dt2 <- read_parquet_dt("output.parquet")
key(dt2)  # Returns c("id", "year")

# Write with custom keys
write_parquet_dt(dt, "output.parquet", keys = c("id"))

# Write with IMPACTncd key logic
dt_impact <- data.table(
  year = 2020:2024, age = 30:34, sex = "M",
  mu = rnorm(5), sigma = runif(5)
)
write_parquet_dt(dt_impact, "output.parquet", keys = "impactncd")
# Keys will be c("age", "sex", "year") - mu/sigma excluded, year moved to end

# Write with partitioning by year
write_parquet_dt(dt, "output_dir", partitioning = "year")

# Auto-partition by year if column exists
write_parquet_dt(dt, "output_dir", partitioning = TRUE)

# Read from fst and write to parquet
write_parquet_dt("data.fst", "output.parquet", keys = c("id", "year"))
}

}
\seealso{
[read_parquet_dt()] for reading parquet files with key restoration.
}
